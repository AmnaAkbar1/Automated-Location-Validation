# -*- coding: utf-8 -*-
"""projectuber.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hyK5sVnskl9n8fQeDTNwProqGIBCUVCx
"""

!pip install selenium
from bs4 import BeautifulSoup
from selenium import webdriver
import time
from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

dataset = pd.read_csv("/content/drive/MyDrive/Dataset/dataset - dataset.csv")
dataset.head()

!apt update
!apt install chromium-chromedriver
!pip install selenium

from selenium import webdriver
from bs4 import BeautifulSoup

def driversetup():
    options = webdriver.ChromeOptions()
    #run Selenium in headless mode
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    #overcome limited resource problems
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("lang=en")
    #open Browser in maximized mode
    options.add_argument("start-maximized")
    #disable infobars
    options.add_argument("disable-infobars")
    #disable extension
    options.add_argument("--disable-extensions")
    options.add_argument("--incognito")
    options.add_argument("--disable-blink-features=AutomationControlled")

    driver = webdriver.Chrome(options=options)

    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")

    return driver

l=list()
o={}
driver = driversetup()
def pagesource(url, driver):
    driver.get(url)
    time.sleep(5)
    resp = driver.page_source
    driver.close()
    soup=BeautifulSoup(resp,'html.parser')
    items = soup.find_all('div',{'class':'x9f619 x1n2onr6 x1ja2u2z x78zum5 x2lah0s x1qughib x1qjc9v5 xozqiw3 x1q0g3np x1pi30zi x1swvt13 xyamay9 xykv574 xbmpl8g x4cne27 xifccgj'})[1]

    allDetails = items.find_all("div",{"class":"x9f619 x1n2onr6 x1ja2u2z x78zum5 x2lah0s x1nhvcw1 x1qjc9v5 xozqiw3 x1q0g3np xyamay9 xykv574 xbmpl8g x4cne27 xifccgj"})

    for contact in allDetails:
        checkaddress = len(contact.text.split(","))
        if(checkaddress>2):
            try:
                o["address"]=contact.text
            except:
                o["address"]=None
            continue

        checknumber = len(contact.text.split("-"))
        if(checknumber>2):
            try:
                o["number"]=contact.text
            except:
                o["number"]=None
            continue

        if('@' in contact.text):
            try:
                o["email"]=contact.text
            except:
                o["email"]=None
            continue
    l.append(o)
    print(l)

    return soup


target_url = input("Enter Target URL: ")
pagesource(target_url, driver)

k = pd.DataFrame(l)
#target_url = "https://www.facebook.com/thelinkersgroup"
k.head()

dataset['address'] = dataset['Company Address']
dataset['address 2'] = k['address']
dataset['New Address'] = ""
dataset['matched address'] =""

# if (dataset['address'] == dataset['address 2']).any() == True:
  #dataset['matched address'] = dataset['address 2']

#if (dataset['address'] == dataset['address 2']).any() == False:
  #dataset['New Address'] = dataset['address 2']

import numpy as np

dataset['New Address'] = [y if x != '' else x for x,y in zip(dataset['address'],dataset['address 2'])]
dataset['matched address'] = [y if x == '' else np.nan for x,y in zip(dataset['address'],dataset['address 2'])]

dataset[['Company Name','Company Address','matched address','New Address']]

